# Knowledge based Language Modelling

## Motivation: 
Natural Language understanding (NLU) has advanced significantly in the last decade. One of the achievements that have fueled these advancements is word embeddings. Word embeddings serve to represent words and capture their semantic meaning in NLU tasks. Learning these embeddings is done by training a classifier on a large text corpora to [predict context](https://arxiv.org/pdf/1310.4546.pdf) words given a word and vice-versa. Word2Vec was the first word embeddings trained in such a way and have been shown to capture semantic meaning very well. Further work has produced [FastText]((https://arxiv.org/pdf/1607.04606.pdf)) which were shown to capture more of the syntactic information as they relied on n-gram features within the word. Recently, [Elmo](https://arxiv.org/pdf/1802.05365.pdf) word embeddings were introduced where words are formed by concatenating their constituent character embeddings which are in turn a function of the entire corpus they belong to. Elmo embeddings circumvent the problem of out of vocabulary embeddings by relaying on character embeddings. The most successful word embeddings so far have been the contextualised word embeddings [BERT](https://arxiv.org/pdf/1810.04805.pdf). BERT uses multiple layers of multi-head transformer architecture - mainly multiple self attention layers - over the sentence' words in order to produce contextualised word embeddings. BERT achieves state of the art in many NLU tasks.

Despite all of the previous efforts, none of these approaches tries to explicitly capture semantic relations, factual knowledge or knowledge contained in knowledge bases when learning the representation for these words. 

